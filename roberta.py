# -*- coding: utf-8 -*-
"""Roberta.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15oxZkoQPDaq_hKwKYTEqhCspTx6ZpZ-L
"""

!pip install -U transformers
!pip install -U accelerate
!pip install -U datasets
!pip install evaluate
!pip install seqeval

import pandas as pd
from datasets import load_dataset
from tqdm.auto import tqdm as tqdmn
from transformers import RobertaTokenizer, RobertaForTokenClassification, TrainingArguments, Trainer, pipeline
from torch.utils.data import DataLoader
import torch.optim as optim
from tqdm import tqdm
from accelerate import Accelerator
import evaluate
import numpy as np

from transformers import AutoTokenizer
from transformers import DataCollatorForTokenClassification
from torch.utils.tensorboard import SummaryWriter
import os

dataset = load_dataset("conll2003")
# Cut the training split in half. I AINT GOT ALL DAY
dataset["train"] = dataset["train"].train_test_split(test_size=0.5)["train"]
dataset

half_length = len(dataset['train']) // 2
half_dataset = {
    'train': load_dataset('conll2003', split=f'train[:{half_length}]'),
    'validation': dataset['validation'],
    'test': dataset['test']
}

half_dataset['train'].features

pd.DataFrame(half_dataset['train'][:])

pd.DataFrame(half_dataset['train'][:])[['tokens', 'ner_tags']].iloc[0]

tags = half_dataset['train'].features['ner_tags'].feature

index2tag = {idx:tag for idx, tag in enumerate(tags.names)}
tag2index = {tag:idx for idx, tag in enumerate(tags.names)}
index2tag

def create_tag_names(example):
    # Your logic for creating tag names
    modified_example = example
    return modified_example

# Apply the function to the 'train' split of the dataset
half_dataset['train'] = half_dataset['train'].map(create_tag_names)

from transformers import AutoTokenizer

model_checkpoint = "roberta-base"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)

inputs = half_dataset['train'][0]['tokens']
inputs = tokenizer(inputs, is_split_into_words=True)
print(inputs.tokens())



print(half_dataset['train'][0]['tokens'])
print(half_dataset['train'][0]['ner_tags'])

inputs.word_ids()

def align_labels_with_tokens(labels, word_ids):
    new_labels = []
    current_word = None
    for word_id in word_ids:
        if word_id != current_word:
            current_word = word_id
            label = -100 if word_id is None else labels[word_id]
            new_labels.append(label)

        elif word_id is None:
            new_labels.append(-100)

        else:
            label = labels[word_id]

            if label % 2 == 1:
                label = label + 1
            new_labels.append(label)

    return new_labels

labels = half_dataset['train'][0]['ner_tags']
word_ids = half_dataset['train'][0]['tokens']
print(labels, word_ids)

def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(examples['tokens'], truncation=True, is_split_into_words=True)

    all_labels = examples['ner_tags']

    new_labels = []
    for i, labels in enumerate(all_labels):
        word_ids = tokenized_inputs.word_ids(i)
        new_labels.append(align_labels_with_tokens(labels, word_ids))

    tokenized_inputs['labels'] = new_labels

    return tokenized_inputs

print("Before mapping:")
for i in range(min(5, len(half_dataset['train']))):  # Print the first 5 examples or all if less than 5
    print(half_dataset['train'][i]['tokens'], half_dataset['train'][i]['ner_tags'])

# !rm -r /content/drive/*
# drive.mount('/content/drive')

from datasets import DatasetDict

# Assuming 'tokenize_and_align_labels' is a function you want to apply to each split
tokenized_half_dataset = {}

for split in half_dataset.keys():
    # Tokenize and align labels
    tokenized_data = half_dataset[split].map(tokenize_and_align_labels, batched=True)

    # If you want to remove columns, specify the list of columns to keep
    columns_to_keep = ['input_ids', 'attention_mask', 'labels']  # Adjust as needed
    tokenized_data = tokenized_data.remove_columns([col for col in tokenized_data.column_names if col not in columns_to_keep])

    tokenized_half_dataset[split] = tokenized_data

# Create a DatasetDict for the tokenized half dataset
tokenized_half_dataset_dict = DatasetDict(tokenized_half_dataset)

# Print the structure of the tokenized half dataset
print(tokenized_half_dataset_dict)

from transformers import DataCollatorForTokenClassification

data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)

# Assuming you have tokenized datasets as 'tokenized_half_dataset_dict'
batch = data_collator([tokenized_half_dataset_dict['train'][i] for i in range(2)])
batch

import evaluate
metric = evaluate.load('seqeval')

ner_feature = half_dataset['train'].features['ner_tags']
ner_feature

label_names = ner_feature.feature.names
label_names

labels = half_dataset['train'][0]['ner_tags']
labels = [label_names[i] for i in labels]
labels

predictions = labels.copy()
predictions[2] = "O"

metric.compute(predictions=[predictions], references=[labels])

def compute_metrics(eval_preds):
  logits, labels = eval_preds

  predictions = np.argmax(logits, axis=-1)

  true_labels = [[label_names[l] for l in label if l!=-100] for label in labels]

  true_predictions = [[label_names[p] for p,l in zip(prediction, label) if l!=-100]
                      for prediction, label in zip(predictions, labels)]

  all_metrics = metric.compute(predictions=true_predictions, references=true_labels)

  return {"precision": all_metrics['overall_precision'],
          "recall": all_metrics['overall_recall'],
          "f1": all_metrics['overall_f1'],
          "accuracy": all_metrics['overall_accuracy']}

id2label = {i:label for i, label in enumerate(label_names)}
label2id = {label:i for i, label in enumerate(label_names)}

print(id2label)

model_checkpoint = "roberta-base"

from transformers import AutoModelForTokenClassification

model_checkpoint = "roberta-base"  # Specify the desired model checkpoint
model = AutoModelForTokenClassification.from_pretrained(
    model_checkpoint, id2label=id2label, label2id=label2id
)

model.config.num_labels

from transformers import Trainer, TrainingArguments
import torch

# Set the output directory in my Google Drive
output_directory = '/content/drive/MyDrive/fine_tuned'

args = TrainingArguments(
    output_dir=output_directory,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
)

# Assuming 'tokenized_datasets' is your half dataset
trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized_half_dataset['train'],
    eval_dataset=tokenized_half_dataset['validation'],
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    tokenizer=tokenizer
)

trainer.train()

# Save the fine-tuned model and tokenizer
model.save_pretrained('/content/drive/MyDrive/fine_tuned')
tokenizer.save_pretrained('/content/drive/MyDrive/fine_tuned')

from transformers import pipeline

checkpoint = "/content/drive/MyDrive/fine_tuned/checkpoint-5265"
token_classifier = pipeline(
    "token-classification", model=checkpoint, aggregation_strategy="simple"
)
token_classifier("My name is Klaus Mustermann. I live in Vienna and work for Erste Bank")